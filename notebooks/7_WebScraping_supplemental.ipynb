{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping and APIs\n",
    "\n",
    "In this notebook, we learn how to scrape data from the Web and get an idea of what Applicaiton Programming Interfaces are (APIs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "**Web Scraping** is a technique for the extraction of information from websites by transforming unstructured data (HTML pages) into structured data (databases or spreadsheets). \n",
    "\n",
    "Even if scraping can be manually performed by a user, it is usually implemented using a **web crawler** (i.e., it is usually implemented as an automatic process). For larger scale scraping see, e.g., [Scrapy](https://scrapy.org).\n",
    "\n",
    "Note that many websites of large platforms such as Facebook do not allow crawling (in the case of Facebook, not without explicit permission)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, large platforms usually make their data available through an **API**s (Application Programming Interface), such as *Facebook*, *Google* and *Twitter*. **More below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of HTML\n",
    "\n",
    "The **HyperText Markup Language (HTML)** is the standard **descriptive markup** language for web pages.\n",
    "\n",
    "\n",
    "- **Markup** language: a human-readable, explicit system for annotating the content of a document. Markdown is another markup language.\n",
    "\n",
    "\n",
    "- **Descriptive** markup languages (e.g. HTML, XML) are used to annotate the structure or the contents of a document, as opposed to **procedural** markup languages (e.g. TEX, Postscript), whose main goal is to describe how a document should be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML provides a means to annotate the <strong>structural</strong> elements of documents like (different kinds of) headings, paragraphs, lists, links, images, quotes, tables and so forth. Similarly, even if with fewer options, does Markdown (which we are <em>using</em> *here*, check the code!).\n",
    "\n",
    "HTML tags **do not mark the logical structure** of a document, but only its format (e.g. *this is a table*, *this is a h3-type heading*...). It is up to the browser to then use HTML (plus other information, such as *Cascading Style Sheets*), to render a webpage appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML markup relies on a **fixed inventory of tags**, written by using angle brackets. Some tags, e.g. `<p>...</p>`, surround the marked text, and may include subelements. Other tags, e.g. `<br>` or `<img>` introduce content directly.\n",
    "\n",
    "The following is an example of a web page:\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>The Adventures of Pinocchio</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h2>Carlo Collodi</h2>\n",
    "    <h1>The Adventures of Pinocchio</h1>\n",
    "    <hr>\n",
    "    <h4>CHAPTER 1</h4>\n",
    "    <br>\n",
    "    <p><i>How it happened that Mastro Cherry, carpenter, found a piece of wood that wept and laughed like a child</i></p>\n",
    "    <br>\n",
    "    <p>Centuries ago there lived--</p>\n",
    "    <p>\"A king!\" my little readers will say immediately.</p>\n",
    "  </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Web Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The following notes are roughly based on the **Chapters 1-3** of: Mitchell, R. (2015). [Web Scraping with Python](http://shop.oreilly.com/product/0636920034391.do), O'Reilly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modules and Packages Required for Web Scraping\n",
    "\n",
    "**BeautifulSoup**: this library defines [classes and functions](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to pull data (e.g. table, lists, paragraphs) out of HTML and XML files. It provides idiomatic ways of navigating, searching, and modifying the parse tree.\n",
    "\n",
    "\n",
    "**lxml**: to function, BeautifulSoup relies on external HTML-XML parsers. Many options are available, among which the html5lib's and the Python's built-in parsers. We'll rely on the [lxml](http://lxml.de/)'s parser, due to its high performance, reliability and flexibility.\n",
    "\n",
    "\n",
    "**Urllib**: BeautifulSoup does not fetch the web page for us. To do this, we'll rely on the [Urllib](https://docs.python.org/3.7/library/urllib.html#module-urllib) module available in the Python Standard Library, that implements classes and functions which help in opening URLs (authentication, redirections, cookies and so on). We will see another option, **requests**, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve and Parse an HTML page\n",
    "\n",
    "`urllib.request.urlopen()` allows us to retrieve our target HTML page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the page doesn't exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    html = urlopen(\"http://www.pythonscraping.com/pages/page.html\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, let's handle this properly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    html = urlopen(\"http://www.pythonscraping.com/pages/page.html\")\n",
    "except urllib.request.URLError as e:\n",
    "    pass # code your plan B here\n",
    "except urllib.request.URLError as e:\n",
    "    raise # raise any other exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `BeautifulSoup()` in conjunction with `lxml` to parse out `html` page and store it in the Beautiful Soup format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might need to to the following:\n",
    "#!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "soup_page1 = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's scrape another couple of pages we'll need in our examples\n",
    "soup_page3 = BeautifulSoup(urlopen(\"http://www.pythonscraping.com/pages/page3.html\"), \"lxml\")\n",
    "soup_wap = BeautifulSoup(urlopen(\"http://www.pythonscraping.com/pages/warandpeace.html\"), \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the nested structure of the page\n",
    "\n",
    "The `prettify()` method allows us to have a look at the structure of the HTML page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>A Useful Page</title>\n",
      "</head>\n",
      "<body>\n",
      "<h1>An Interesting Title</h1>\n",
      "<div>\n",
      "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup_page1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   A Useful Page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   An Interesting Title\n",
      "  </h1>\n",
      "  <div>\n",
      "   Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup_page1.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's play with a HTML tag\n",
    "\n",
    "The notation `soup.<tag>` allows us to retrieve the content marked by a tag (opening and closing tags included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div>\n",
       "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that the first \"<div>\" tag is nested two layers deep (html ‚Üí body ‚Üí div).\n",
    "soup_page1.div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the text is the only thing you're interested into, well, the `soup.<tag>.string` method comes in handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_page1.div.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTML markup generated by Beautiful Soup can be modified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's change the content of our div\n",
    "soup_page1.div.string = \"this content has been changed\"\n",
    "# let's change the name of the tag\n",
    "soup_page1.div.name = \"new_div\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup_page1.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its simplest use, the `find()` method is an alternative to the `soup.<tag>` notation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_page1.find(\"new_div\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_page1.new_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but this function allows for the searching of nodes by exploiting cues in the markup, such as a given **class attribute** value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup_wap.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_wap.find(\"span\", attrs = {\"class\":\"green\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of an attribute for a given tag instance can be retrieved by using the `get(\"ATTRIBUTE\")` method. For instance, if we want to retrieve the URL of an image we can extract the `src` value from the corresponding `<img>` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_page3.img.get(\"src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to know all the attibutes associated with a given tag, the `attrs` method is convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_page3.img.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by returning a dictionary, it is easy to see how \"attrs\" can be used as an alternative to \"get()\"\n",
    "soup_page3.img.attrs[\"src\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you fancy another way to do the same thing...\n",
    "soup_page3.img[\"src\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with multiple HTML tags at once\n",
    "\n",
    "When the same tag is used multiple time in the same page, however, both the `soup.<tag>` notation and the `find()` method allow you to access **only one instance** (i.e. the first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup_wap.prettify())[180:1190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_wap.span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract the **sequence of all the instances of a tag** in a file, we can use the `find_all()` method (previously known as `findAll()` and `findChildren()` in BS 3 and BS 2, respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_wap.find_all(\"span\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `find_all()` method as well allows for  the extraction of  all tags by exploiting cues in the markup, such as a given **class attribute** value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_wap.find_all(\"span\",  attrs = {\"class\":\"green\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Crawling\n",
    "\n",
    "Web Crawlers are softwares designed to collect pages from the Web. In essence, they recursively implement the following steps: \n",
    "\n",
    "- they start by retrieving the page content for an URL \n",
    "\n",
    "\n",
    "- they then parse it to retrieve other URLs of interest\n",
    "\n",
    "\n",
    "- they then focus on these new URLs, for each of which they repeat the whole process, ad infinitum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if you want to crawl and **entire site**:\n",
    "\n",
    "- start with a top-level page\n",
    "\n",
    "\n",
    "- parse the page (retrieve the data your application need) and extract all the internal links, by ignoring already visited URLs\n",
    "\n",
    "\n",
    "- for each new link, move to the corresponding page and repeat the previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Random walk through Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set our starting page URL, fetch it and parse its HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_page = urlopen(\"https://en.wikipedia.org/wiki/Chris_Cornell\")\n",
    "soup = BeautifulSoup(starting_page, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it should be easy to extract all the links in the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links are defined by <a> tag\n",
    "for link_element in soup.find_all(\"a\")[:10]:\n",
    "    print(link_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ignore all the \"a\" tags without an \"href\" attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link_element in [tag for tag in soup.find_all(\"a\") if 'href' in tag.attrs][:10]:\n",
    "    \n",
    "    url = link_element.attrs['href']\n",
    "    \n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia is full of sidebar, footer, and header links that appear on every page, along with links to the category pages, talk pages, and other pages that do not contain different articles:\n",
    "\n",
    "```\n",
    "/wiki/Template_talk:Chris_Cornell\n",
    "```\n",
    "\n",
    "```\n",
    "#cite_note-147\n",
    "```\n",
    "\n",
    "Moreover, we don't want to visit pages outside of Wikipedia:\n",
    "\n",
    "```\n",
    "http://www.chriscornell.com/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant links have three thing in common:\n",
    "\n",
    "- they reside within the `div` with the `id` set to `bodyContent`\n",
    "\n",
    "\n",
    "- the URLs do not contain semicolons\n",
    "\n",
    "\n",
    "- the URLs begin with `/wiki/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "re_pattern = re.compile(r\"^(/wiki/)((?!:).)*$\")\n",
    "\n",
    "body = soup.find(\"div\", {\"id\": \"bodyContent\"})\n",
    "\n",
    "for link in body.find_all(\"a\", {'href': re_pattern}):\n",
    "\n",
    "    print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code returns the list of all the Wikipedia articles linked to our starting page. \n",
    "\n",
    "This is not enough, we want to be recursively repeat this process for all these links. That is, we need a function that takes as input a Wikipedia article URL of the form `/wiki/<Article_Name>` and returns a list of all linked articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(article_url):\n",
    "    \"\"\"\n",
    "    Retrieve all URLs on an English Wikipedia article page (e.g. /wiki/Amsterdam).\n",
    "    \n",
    "    This function needs a relative URL on the \n",
    "    http://en.wikipedia.org domain, such as '/wiki/Amsterdam'. \n",
    "    \n",
    "    Args:\n",
    "        article_url (str): URL of a website\n",
    "        \n",
    "    Returns:\n",
    "        bs4.element.ResultSet: bs link elements resultset\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    page = urlopen(\"http://en.wikipedia.org\" + article_url)\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    body = soup.find(\"div\", {\"id\":\"bodyContent\"})\n",
    "    \n",
    "    re_pattern = re.compile(r\"^(/wiki/)((?!:).)*$\")\n",
    "    \n",
    "    links = body.find_all(\"a\", href=re_pattern)\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function by calling it in a script that randomly select, for each iteration, a random link and that stops after 10 URLs have been retrieved (or when it bumps into a page without link):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "links = get_links(\"/wiki/Chris_Cornell\")\n",
    "\n",
    "for _ in range(10):  # for testing purposes, we want to do this 10 times\n",
    "    if len(links) > 0:\n",
    "        new_article = links[random.randint(0, len(links)-1)].attrs[\"href\"]\n",
    "        print(new_article)\n",
    "        \n",
    "        links = get_links(new_article)\n",
    "        \n",
    "    else:\n",
    "        print(\"No links in this page!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.\n",
    "\n",
    "Write code to retrieve the **motto in English** (if the University has one) of the Internationally Ranked Universities in the Netherlands by starting from the following Wikipedia article:\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_universities_in_the_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with APIs\n",
    "\n",
    "An **Application Programming Interface** is a set of protocols that defines how software programs communicate among eachother. Without APIs, we have to scrape the Web or get the data directly. With APIs, we often can get structured data: it is a much more convenient way to work.\n",
    "\n",
    "APIs are a great option in that they implement extensively tested routines (**high reliability**). However, you should spend time in learning how they work and, in some cases, they don't allow you to access the piece of information you may need (**low flexibility**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # External package: https://requests.readthedocs.io/en/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a Google search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tesla\"\n",
    "r = requests.get('https://www.google.com/search', params={'q': query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text/html; charset=utf-8\n",
      "utf-8\n",
      "https://consent.google.com/ml?continue=https://www.google.com/search%3Fq%3DTesla&gl=NL&m=0&pc=srp&uxe=none&hl=nl&src=1\n"
     ]
    }
   ],
   "source": [
    "print(r.headers['content-type'])\n",
    "print(r.encoding)\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><html lang=\"nl\" dir=\"ltr\"><head><style nonce=\"fZ31PIV4esqLvwEfQARW2w\">\\na, a:link, a:visited, a:active, a:hover {\\n  color: #1a73e8;\\n  text-decoration: none;\\n}\\nbody {\\n  font-family: Roboto,RobotoDraft,Helvetica,Arial,sans-serif;\\n  text-align: center;\\n  -ms-text-size-adjust: 100%;\\n  -moz-text-size-adjust: 100%;\\n  -webkit-text-size-adjust: 100%;\\n}\\n.box {\\n  border: 1px solid #dadce0;\\n  box-sizing: border-box;\\n  border-radius: 8px;\\n  margin: 24px auto 5px auto;\\n  max-width: 800px;\\n  padding: 24px;\\n}\\nh1 {\\n  color: #2c2c2c;\\n  font-size: 24px;\\n  hyphens: auto;\\n  margin: 24px 0;\\n}\\n.icaCallout {\\n  background-color: #f8f9fa;\\n  padding: 12px 16px;\\n  border-radius: 10px;\\n  margin-bottom: 10px;\\n}\\np, .sub, .contentText, .icaCallout {\\n  color: #5f6368;;\\n  font-size: 14px;\\n  line-height: 20px;\\n  letter-spacing: 0.2px;\\n  text-align: left;\\n}\\n.signin {\\n  text-align: right;\\n}\\n.saveButtonContainer,\\n.saveButtonContainerNarrowScreen {\\n  width: 100%;\\n  margin-top: 12px;\\n}\\n.customButtonContainer {'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.\n",
    "\n",
    "1. Inspect the Google search results page and understand how results are displayed.\n",
    "\n",
    "\n",
    "2. Use BeautifulSoup to get the link of the first 10 results of this search out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about using `requests` to query APIs? Easy using the param dictionary. Responses then follow the starndard format of the API (or you can request the one you like if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://api.github.com')\n",
    "\n",
    "# raw\n",
    "r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API (OPT)\n",
    "\n",
    "Two main APIs:\n",
    "\n",
    "* **Streaming API**: a sample of public tweets and events as they published on Twitter, provides only real-time data without limits.\n",
    "\n",
    "\n",
    "* **REST API**: allows to search, follow trends, read author profile and follower data, post / modify. It provides historical data up to a week (for the free account, more by paying), requires a one-time request and has rate limit (varies for different requests and subscriptions).\n",
    "\n",
    "\n",
    "REST APIs (it is a style for developing Web services which is widely used): https://en.wikipedia.org/wiki/Representational_state_transfer\n",
    "\n",
    "Some more basic info: https://developer.twitter.com/en/docs/basics/things-every-developer-should-know\n",
    "\n",
    "Tutorials: https://developer.twitter.com/en/docs/tutorials\n",
    "\n",
    "Note that Twitter changed to a new version of their API (Twitter API v2) about two years ago. So any tutorials you can find from before that will not work for you. This one is updated for 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the API: authentication\n",
    "\n",
    "**For this part, you will need credentials from the Twitter dev website.**\n",
    "\n",
    "A good way to store your keys is using `.conf` files and `configparser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stuff/conf.conf']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "config = configparser.RawConfigParser()\n",
    "config.read(\"stuff/conf.conf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6AJDQJhBp3MX4nIXKd108Piyt'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['twitter']['api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Section: twitter>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['twitter']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how my `conf.conf` file looks like (also in `stuff/conf_public.conf`):\n",
    "\n",
    "```\n",
    "[twitter]\n",
    "api_key = YOURS\n",
    "api_secret_key = YOURS\n",
    "access_token = YOURS\n",
    "access_secret_token = YOURS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A useful package: Tweepy\n",
    "\n",
    "https://tweepy.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get errors about the Client object not having certain attributes, you may have to update Tweepy. Since this Twitter API v2 is relatively new, you need a new version of Tweepy to work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tweepy==4.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @BibliotheekUvA: Op woensdag 15 maart zijn de Provinciale Staten- en Waterschapsverkiezingen. De bibliotheeklocatie UB Singel is die dag‚Ä¶\n",
      "Hoeveel geld hebben de politieke partijen uitgegeven aan hun online campagnes voor de Provinciale Statenverkiezingen? Het online dashboard wordt dagelijks bijgewerkt! \n",
      "#ps2023 #verkiezingen15maart #verkiezingen https://t.co/Dc0NW18sX8\n",
      "In Open Kaart‚Äì van atlas tot streetmap reis je door zeven eeuwen kaarten en atlassen √©n werp je een blik in de toekomst. Bekijk een enorme diversiteit aan kaarten vanaf de 15de eeuw en ontdek de verhalen die erachter schuilgaan in het @allard_pierson. \n",
      "https://t.co/pQAXXtb5z0\n",
      "Tussen 13 en 17 maart 2023 vindt de jaarlijkse #WeektegenRacisme plaats. Het Centrale Diversity Office (CDO) van de UvA presenteert tijdens deze week een programma dat verschillende aspecten van racisme behandelt. @Machiel_Keestra vertelt over de week:\n",
      "https://t.co/fgVrlfrQNK\n",
      "Algoritmes kunnen forensisch deskundigen ondersteunen bij sprekersherkenning en gezichtsvergelijkingen. Het vandaag geopende onderzoekslab AI4forensics is gericht op¬†#AI in forensische bewijsvoering en is daarmee uniek in zijn soort in Nederland: https://t.co/l8oSCQ8zpy\n",
      "RT @parool: De Universiteit van Amsterdam (UvA) laat haar eigen betrokkenheid bij het Nederlandse koloniale- en slavernijverleden onderzoek‚Ä¶\n",
      "We gaan de betrokkenheid van de UvA bij het Nederlands koloniale- en slavernijverleden onderzoeken. Eerst bekijken we welke collecties, archieven en gebouwen verder moeten worden onderzocht. Daarna begint een groter onderzoek dat enkele jaren zal duren. https://t.co/viu01XMjEA\n",
      "üëÄBinnenkort worden twee beroemde 17e-eeuwse toneelstukken voor het eerst in de geschiedenis samen opgevoerd; √©√©n in het Nederlands en √©√©n in het Engels. The Miller + The King is een samenwerking tussen UvA @UvA_Humanities en @Princeton. https://t.co/eiK1lQaIvB\n",
      "RT @UvA_Humanities: Voor het eerst in de geschiedenis worden de 17e-eeuwse toneelklassiekers De Klucht van de Molenaar &amp; King Lear binnenko‚Ä¶\n",
      "Twee jonge UvA-historici Rukayyah Reichling en Amir Taha @5000Emir zijn @ScienceFaces @_knaw! Via blogs over onderzoek en de wetenschap laten ze zien hoe het leven eruitziet als je een beginnend wetenschapper bent. Gefeliciteerd allebei!üéâ @UvA_Humanities  https://t.co/ZlAiYpPV1N\n"
     ]
    }
   ],
   "source": [
    "# Tweepy Hello World\n",
    "\n",
    "# authentication (OAuth 2.0)\n",
    "client = tweepy.Client(config['twitter']['bearer_token'], wait_on_rate_limit=True)\n",
    "\n",
    "public_tweets = client.get_users_tweets('156280168')\n",
    "for tweet in public_tweets.data:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interlude: JSON\n",
    "\n",
    "The Twitter API returns data structured in the JSON format. [JSON](https://www.json.org) (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate. **It is basically a list of nested Python dictionaries.**\n",
    "\n",
    "\n",
    "Minimal example:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"firstName\": \"John\",\n",
    "  \"lastName\": \"Doe\",\n",
    "  \"age\": 21\n",
    "}\n",
    "```\n",
    "\n",
    "Extended example:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"$id\": \"https://example.com/person.schema.json\",\n",
    "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "  \"title\": \"Person\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"firstName\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The person's first name.\"\n",
    "    },\n",
    "    \"lastName\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The person's last name.\"\n",
    "    },\n",
    "    \"age\": {\n",
    "      \"description\": \"Age in years which must be equal to or greater than zero.\",\n",
    "      \"type\": \"integer\",\n",
    "      \"minimum\": 0\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "Online viewer: http://jsonviewer.stack.hu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the API: search\n",
    "\n",
    "All the most recent Tweets from a given hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @jamescalam: latest video on @LangChainAI covering the new chat objects, prompt templates, etc ‚Äî built for @OpenAI's chat-gpt endpoint (‚Ä¶\n",
      "GPT-4  is better than ChatGPT \n",
      "-&gt; for more details, check https://t.co/k23eHoNh8N\n",
      "\n",
      "#gpt4 #openai #nlproc #gpt3 #chatgpt #nlp https://t.co/fiiUGsrO74\n",
      "RT @AmazonScience: To enable language models that are lightweight enough for runtime use, Amazon scientists propose using an LLM-based teac‚Ä¶\n",
      "Interestingly, GPT-4 paper confirms and studies the data contamination issues on academic benchmarks, but results are difficult to interpret. #NLProc @PanLiangming https://t.co/Pu5gMokbMg\n",
      "Ladies and Gentlemen, it's here... #gpt4 \n",
      "https://t.co/RZmNcGm96h\n",
      "#ChatGPT #AI #NLP2023 #NLProc\n",
      "RT @xposivibesx: Gesucht: Promovierende in Sprachtechnologie oder Computerlinguistik die Lust haben, an der Organisation des PhD Events auf‚Ä¶\n",
      "RT @KirkDBorne: 100+ Free #DataScience eBooks (downloadable PDF) for Beginners and for Experts ‚Äî via @TheInsaneApp \n",
      "‚Äî‚Äî‚Äî‚Äî‚Äî\n",
      "#100DaysOfCode #B‚Ä¶\n",
      "RT @jamescalam: latest video on @LangChainAI covering the new chat objects, prompt templates, etc ‚Äî built for @OpenAI's chat-gpt endpoint (‚Ä¶\n",
      "RT @acl_srw: Thank you to all the participants who submitted their work for pre-submission mentoring at #ACL2023 SRW! We're now calling on‚Ä¶\n",
      "RT @jamescalam: latest video on @LangChainAI covering the new chat objects, prompt templates, etc ‚Äî built for @OpenAI's chat-gpt endpoint (‚Ä¶\n",
      "RT @jamescalam: latest video on @LangChainAI covering the new chat objects, prompt templates, etc ‚Äî built for @OpenAI's chat-gpt endpoint (‚Ä¶\n",
      "RT @jamescalam: latest video on @LangChainAI covering the new chat objects, prompt templates, etc ‚Äî built for @OpenAI's chat-gpt endpoint (‚Ä¶\n",
      "RT @jamescalam: latest video on @LangChainAI covering the new chat objects, prompt templates, etc ‚Äî built for @OpenAI's chat-gpt endpoint (‚Ä¶\n",
      "RT @jamescalam: latest video on @LangChainAI covering the new chat objects, prompt templates, etc ‚Äî built for @OpenAI's chat-gpt endpoint (‚Ä¶\n",
      "RT @jamescalam: latest video on @LangChainAI covering the new chat objects, prompt templates, etc ‚Äî built for @OpenAI's chat-gpt endpoint (‚Ä¶\n",
      "Subscribe now for the latest data science news and articles: https://t.co/A49rTSlh3c #datascience #machinelearning #artificialintelligence #nlproc #deeplearning #bigdata #analytics #dataviz\n",
      "The deadline for this postdoc position in NLP, and human-AI interaction for haptics is extended to May 8, feel free to reach out to me beforehand. I'll be at #chi2023 to chat.@DIKU_Institut @ASUEngineering @SCAI_ASU @daniel_hers #NLProc https://t.co/IQQh1czkJw\n",
      "RT @jamescalam: latest video on @LangChainAI covering the new chat objects, prompt templates, etc ‚Äî built for @OpenAI's chat-gpt endpoint (‚Ä¶\n",
      "RT @fly51fly: [CL] Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling\n",
      "Z Zhang, L Zhou, C Wang, S Che‚Ä¶\n",
      "Related to RLHF method in ChatGPT, 4 years ago I used a policy gradient model for text generation where the reward model was a pretrained semantic textual similarity model that optimized an actor-critic based generator along w/ cross-entropy loss\n",
      "https://t.co/X5ozkP2bWD #NLProc\n",
      "RT @fly51fly: [CL] ChatAug: Leveraging ChatGPT for Text Data Augmentation\n",
      "H Dai, Z Liu, W Liao, X Huang, Z Wu, L Zhao... [University of Geo‚Ä¶\n",
      "RT @fly51fly: [CL] MathPrompter: Mathematical Reasoning using Large Language Models\n",
      "S Imani, L Du, H Shrivastava [Microsoft Research] (2023‚Ä¶\n",
      "RT @fly51fly: [CL] Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference\n",
      "C Wang, S X Liu, A H. Awadallah‚Ä¶\n",
      "RT @fly51fly: [CL] Rewarding Chatbots for Real-World Engagement with Millions of Users\n",
      "R Irvine, D Boubert, V Raina, A Liusie, V Mudupalli,‚Ä¶\n",
      "RT @fly51fly: [CL] Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering -- Ex‚Ä¶\n",
      "RT @fly51fly: [CL] An Overview on Language Models: Recent Developments and Outlook\n",
      "C Wei, Y Wang, B Wang, C.-C. J Kuo [University of Southe‚Ä¶\n",
      "RT @jamescalam: latest video on @LangChainAI covering the new chat objects, prompt templates, etc ‚Äî built for @OpenAI's chat-gpt endpoint (‚Ä¶\n",
      "RT @uld_galway: \"Field Linguistics and NLP: Can they be friends?\" is the title of our next #Cardamom seminar by Dr Ritesh Kumar of @ctrans_‚Ä¶\n",
      "latest video on @LangChainAI covering the new chat objects, prompt templates, etc ‚Äî built for @OpenAI's chat-gpt endpoint (and future chat models...)\n",
      "\n",
      "https://t.co/LvIwYn2a9R\n",
      "\n",
      "#NLProc #GenerativeAI #MachineLearning\n",
      "RT @gp_pulipaka: Zero-Shot Text Classification! #BigData #Analytics #DataScience #AI #MachineLearning #NLProc #IoT #IIoT #Python #RStats #T‚Ä¶\n",
      "RT @uld_galway: \"Field Linguistics and NLP: Can they be friends?\" is the title of our next #Cardamom seminar by Dr Ritesh Kumar of @ctrans_‚Ä¶\n",
      "RT @KirkDBorne: 100+ Free #DataScience eBooks (downloadable PDF) for Beginners and for Experts ‚Äî via @TheInsaneApp \n",
      "‚Äî‚Äî‚Äî‚Äî‚Äî\n",
      "#100DaysOfCode #B‚Ä¶\n",
      "\"Field Linguistics and NLP: Can they be friends?\" is the title of our next #Cardamom seminar by Dr Ritesh Kumar of @ctrans_cl. Sign up at https://t.co/lAYh8JMFuB and join us on March 27th at 5pm Irish time (UTC+01:00). #NLProc #FieldLinguistics @uniofgalway @insight_centre\n",
      "IQVIA‚Äôs experts, Sara Horsfall &amp; Rina Sarkar, were recently featured on @MuleSoft's API Unplugged podcast to discuss the @IQVIA_global API marketplace, considerations for designing #APIs for a positive #dev experience, and more. Listen now: \n",
      " https://t.co/ECFhXMNQcY \n",
      "#NLProc https://t.co/jCrKSwz9VK\n",
      "[GitHub Repo] OpenAI Cookbook\n",
      "https://t.co/UeBerH6ZU7\n",
      "#MachineLearning #NLProc\n",
      "RT @kelina1124: My group is expanding to Europe!üåçü•≥ In July, I will move back to my hometown in Germany to join the faculty at @uni_mainz, w‚Ä¶\n",
      "Google announces PaLM API &amp; MakerSuite: an approachable way to start prototyping and building generative AI applications https://t.co/Nj3aWOdGFC #nlproc #nlp #voicefirst #ChatGPT\n",
      "RT @KirkDBorne: 100+ Free #DataScience eBooks (downloadable PDF) for Beginners and for Experts ‚Äî via @TheInsaneApp \n",
      "‚Äî‚Äî‚Äî‚Äî‚Äî\n",
      "#100DaysOfCode #B‚Ä¶\n",
      "RT @chrisprograms: To all #programming/#nlproc/#AI folks: Any recommended public Twitter lists to stay up-to-date on recent developments? N‚Ä¶\n",
      "To all #programming/#nlproc/#AI folks: Any recommended public Twitter lists to stay up-to-date on recent developments? Naturally, asking for a friend. üëÄ\n",
      "RT @xposivibesx: Gesucht: Promovierende in Sprachtechnologie oder Computerlinguistik die Lust haben, an der Organisation des PhD Events auf‚Ä¶\n",
      "RT @KirkDBorne: 60 #ChatGPT Prompts for #DataScience: https://t.co/WVZoGzTHQA\n",
      "‚Äî‚Äî‚Äî‚Äî\n",
      "#BigData #DataScientists #MachineLearning #DeepLearning‚Ä¶\n",
      "Is #NLProc the right choice for risk adjustment? Our solution enables payers and providers to connect technology with subject matter expertise in an easy-to-use workflow, ensuring a higher level of confidence in #RiskAdjustment submissions. Read more:\n",
      "https://t.co/XenRjeekv8\n",
      "RT @xposivibesx: Gesucht: Promovierende in Sprachtechnologie oder Computerlinguistik die Lust haben, an der Organisation des PhD Events auf‚Ä¶\n",
      "RT @SapienzaNLP: 7 years after its foundation, @Babelscape is doing a great job at tech transfer and industrial research, with great scienc‚Ä¶\n",
      "RT @KirkDBorne: Download the 78-page PDF #ChatGPT Super Cheat Sheet by @DataCamp here: https://t.co/4xhJjB79sZ¬†\n",
      "‚Äî‚Äî‚Äî‚Äî\n",
      "#DataScientists #DataS‚Ä¶\n",
      "RT @evanmiltenburg: Oh hey, @iwcs2023 now has a Twitter account! \n",
      "\n",
      "The CFP was extended to 22 March, so you can still submit: https://t.co/‚Ä¶\n",
      "RT @KirkDBorne: 100+ Free #DataScience eBooks (downloadable PDF) for Beginners and for Experts ‚Äî via @TheInsaneApp \n",
      "‚Äî‚Äî‚Äî‚Äî‚Äî\n",
      "#100DaysOfCode #B‚Ä¶\n",
      "RT @1littlecoder: ü§îDo you want to always rely on OpenAI? Maybe not!\n",
      "\n",
      "‚ö°Ô∏èUse Open Source Models hosted on ü§ó @huggingface with the help of @gp‚Ä¶\n",
      "Oh hey, @iwcs2023 now has a Twitter account! \n",
      "\n",
      "The CFP was extended to 22 March, so you can still submit: https://t.co/j3c9fI8KqH\n",
      "\n",
      "#NLProc\n",
      "RT @Val_D_Richard: Second call for papers: üì¢üìÑ\n",
      " * InqBnB4 workshop on the Semantics of Interrogatives and Inquisitive Logic https://t.co/v0u‚Ä¶\n",
      "Second call for papers: üì¢üìÑ\n",
      " * InqBnB4 workshop on the Semantics of Interrogatives and Inquisitive Logic https://t.co/v0uxBwogAI\n",
      "* submission deadline: 14 April AoE\n",
      "* 20 June in Nancy (France)\n",
      " * hosted by @iwcs2023 \n",
      "@LOGIC_list @RoelofsenFloris @champoll #NLProc https://t.co/CkkC4NxVKZ\n",
      "RT @iwcs2023: #iwcs2023 #NLProc Deadline for paper submission: 15 March 2023 --&gt; 22 March 2023! https://t.co/ZMwzzA4t8E\n",
      "Even though I don't agree with everything in here, an interesting read #ChatGPT #NLProc https://t.co/BEBWiF7Vs8\n",
      "RT @KirkDBorne: 9 Best Free Online Courses on #PyTorch for #DeepLearning in 2023 ‚Äî compiled by @tut_ml \n",
      "‚Äî‚Äî‚Äî‚Äî\n",
      "#BigData #DataScience #AI #Mac‚Ä¶\n",
      "RT @YasminMoslem: Updated version of our paper on real-time adaptive #NMT with LLMs. Major updates: ‚ú≥ #GPT3 vs. BLOOM &amp; BLOOMZ; ‚ú≥ GPT-3 vs.‚Ä¶\n",
      "RT @gp_pulipaka: Zero-Shot Text Classification! #BigData #Analytics #DataScience #AI #MachineLearning #NLProc #IoT #IIoT #Python #RStats #T‚Ä¶\n",
      "Updated version of our paper on real-time adaptive #NMT with LLMs. Major updates: ‚ú≥ #GPT3 vs. BLOOM &amp; BLOOMZ; ‚ú≥ GPT-3 vs. ModernMT; and ‚ú≥ human evaluation of #terminology-constrained MT with GPT-3. #NLProc\n",
      "\n",
      "https://t.co/N1jvTC2ttD\n",
      "RT @KirkDBorne: 100+ Free #DataScience eBooks (downloadable PDF) for Beginners and for Experts ‚Äî via @TheInsaneApp \n",
      "‚Äî‚Äî‚Äî‚Äî‚Äî\n",
      "#100DaysOfCode #B‚Ä¶\n",
      "RT @acl_srw: Thank you to all the participants who submitted their work for pre-submission mentoring at #ACL2023 SRW! We're now calling on‚Ä¶\n",
      "RT @KirkDBorne: 100+ Free #DataScience eBooks (downloadable PDF) for Beginners and for Experts ‚Äî via @TheInsaneApp \n",
      "‚Äî‚Äî‚Äî‚Äî‚Äî\n",
      "#100DaysOfCode #B‚Ä¶\n",
      "RT @gp_pulipaka: Zero-Shot Text Classification! #BigData #Analytics #DataScience #AI #MachineLearning #NLProc #IoT #IIoT #Python #RStats #T‚Ä¶\n",
      "RT @kelina1124: My group is expanding to Europe!üåçü•≥ In July, I will move back to my hometown in Germany to join the faculty at @uni_mainz, w‚Ä¶\n",
      "RT @KirkDBorne: 8 Best Advanced #DeepLearning Courses Online You Must Know in 2023: https://t.co/2tVrL5uMwF compiled by @tut_ml \n",
      "‚Äî‚Äî‚Äî‚Äî\n",
      "#BigD‚Ä¶\n",
      "RT @KirkDBorne: 8 Best Advanced #DeepLearning Courses Online You Must Know in 2023: https://t.co/2tVrL5uMwF compiled by @tut_ml \n",
      "‚Äî‚Äî‚Äî‚Äî\n",
      "#BigD‚Ä¶\n",
      "RT @KirkDBorne: Download the 78-page PDF #ChatGPT Super Cheat Sheet by @DataCamp here: https://t.co/4xhJjB79sZ¬†\n",
      "‚Äî‚Äî‚Äî‚Äî\n",
      "#DataScientists #DataS‚Ä¶\n",
      "RT @KirkDBorne: Download the 78-page PDF #ChatGPT Super Cheat Sheet by @DataCamp here: https://t.co/4xhJjB79sZ¬†\n",
      "‚Äî‚Äî‚Äî‚Äî\n",
      "#DataScientists #DataS‚Ä¶\n",
      "RT @KirkDBorne: 100+ Free #DataScience eBooks (downloadable PDF) for Beginners and for Experts ‚Äî via @TheInsaneApp \n",
      "‚Äî‚Äî‚Äî‚Äî‚Äî\n",
      "#100DaysOfCode #B‚Ä¶\n",
      "RT @gp_pulipaka: Zero-Shot Text Classification! #BigData #Analytics #DataScience #AI #MachineLearning #NLProc #IoT #IIoT #Python #RStats #T‚Ä¶\n",
      "RT @gp_pulipaka: Zero-Shot Text Classification! #BigData #Analytics #DataScience #AI #MachineLearning #NLProc #IoT #IIoT #Python #RStats #T‚Ä¶\n",
      "RT @gp_pulipaka: Zero-Shot Text Classification! #BigData #Analytics #DataScience #AI #MachineLearning #NLProc #IoT #IIoT #Python #RStats #T‚Ä¶\n",
      "RT @gp_pulipaka: Zero-Shot Text Classification! #BigData #Analytics #DataScience #AI #MachineLearning #NLProc #IoT #IIoT #Python #RStats #T‚Ä¶\n",
      "Zero-Shot Text Classification! #BigData #Analytics #DataScience #AI #MachineLearning #NLProc #IoT #IIoT #Python #RStats #TensorFlow #Java #JavaScript #ReactJS #GoLang #CloudComputing #Serverless #DataScientist #Linux #Programming #Coding #100DaysofCode \n",
      "https://t.co/gZoW0JFdWZ https://t.co/fbkRUUHhGI\n",
      "RT @acl_srw: Thank you to all the participants who submitted their work for pre-submission mentoring at #ACL2023 SRW! We're now calling on‚Ä¶\n",
      "Thank you to all the participants who submitted their work for pre-submission mentoring at #ACL2023 SRW! We're now calling on the more experienced members of the #NLProc community to help guide these students as mentors. Reach out to acl2023.srw@gmail.com if you are interested!\n",
      "https://t.co/hZQsiEL5qF Consistency Analysis of ChatGPT. (arXiv:2303.06273v1 [https://t.co/HW5RVw4UkE]) #NLProc\n",
      "https://t.co/RI9PQNVkLY Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG]) #NLProc\n",
      "https://t.co/A7RsPnI3F4 Parachute: Evaluating Interactive Human-LM Co-writing Systems. (arXiv:2303.06333v1 [cs.HC]) #NLProc\n",
      "https://t.co/ZsTNxlWcKa ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation. (arXiv:2303.06458v1 [https://t.co/HW5RVw4UkE]) #NLProc\n",
      "https://t.co/PT46tPanMt Transcription free filler word detection with Neural semi-CRFs. (arXiv:2303.06475v1 [https://t.co/gWjzhuyqks]) #NLProc\n",
      "https://t.co/2lnYmQ9YCA Pre-trained Language Models in Biomedical Domain: A Systematic Survey. (arXiv:2110.05006v3 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc\n",
      "https://t.co/Sgy3AO3cWt Two-view Graph Neural Networks for Knowledge Graph Completion. (arXiv:2112.09231v4 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc\n",
      "https://t.co/EXo4QIQCe7 Temporal Sentence Grounding in Videos: A Survey and Future Directions. (arXiv:2201.08071v3 [https://t.co/Ee5Iq68M0E] UPDATED) #NLProc\n",
      "https://t.co/Rh3v2Uo4zj I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image Captioning. (arXiv:2202.06574v3 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc\n",
      "https://t.co/ZOp19eMNrZ Alternate Intermediate Conditioning with Syllable-level and Character-level Targets for Japanese ASR. (arXiv:2204.00175v2 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc\n",
      "https://t.co/exalOOCaVC A Token-level Contrastive Framework for Sign Language Translation. (arXiv:2204.04916v2 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc\n",
      "https://t.co/kelPQPkZLq EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language Processing. (arXiv:2205.00258v2 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc\n",
      "https://t.co/fFxq2z6JtD DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech. (arXiv:2207.01063v3 [https://t.co/gWjzhuyqks] UPDATED) #NLProc\n",
      "https://t.co/fPS4XFWyGw Learning ASR pathways: A sparse multilingual ASR model. (arXiv:2209.05735v3 [https://t.co/gWjzhuyqks] UPDATED) #NLProc\n",
      "https://t.co/3X5pI7zm3m Assessing the impact of contextual information in hate speech detection. (arXiv:2210.00465v3 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc\n",
      "https://t.co/dhYiYvwfWX The Surprising Computational Power of Nondeterministic Stack RNNs. (arXiv:2210.01343v3 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc\n",
      "https://t.co/Qz57IDsexH EDU-level Extractive Summarization with Varying Summary Lengths. (arXiv:2210.04029v2 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc\n",
      "https://t.co/soePVGLE1c Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective. (arXiv:2210.10488v4 [https://t.co/HW5RVw5sac] UPDATED) #NLProc\n",
      "https://t.co/eEXmcXqiX0 Augmentation with Projection: Towards an Effective and Efficient Data Augmentation Paradigm for Distillation. (arXiv:2210.11768v2 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc\n",
      "https://t.co/nc7g5B1oVb Named Entity Detection and Injection for Direct Speech Translation. (arXiv:2210.11981v2 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc\n",
      "https://t.co/kZCClq3ZMI Articulation GAN: Unsupervised modeling of articulatory learning. (arXiv:2210.15173v2 [https://t.co/guu9Y4Sk4n] UPDATED) #NLProc\n",
      "https://t.co/dlPzjckkyp Leveraging Label Correlations in a Multi-label Setting: A Case Study in Emotion. (arXiv:2210.15842v2 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc\n",
      "https://t.co/9m4wusTp8U Using Emotion Embeddings to Transfer Knowledge Between Emotions, Languages, and Annotation Formats. (arXiv:2211.00171v2 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc\n",
      "https://t.co/pcvHWUurSs BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. (arXiv:2211.05100v3 [https://t.co/HW5RVw4UkE] UPDATED) #NLProc\n",
      "https://t.co/HTZUsJPQGl Accidental Learners: Spoken Language Identification in Multilingual Self-Supervised Models. (arXiv:2211.05103v2 [https://t.co/gWjzhuyqks] UPDATED) #NLProc\n"
     ]
    }
   ],
   "source": [
    "# queries\n",
    "\n",
    "tweets = client.search_recent_tweets(query='#nlproc', max_results=100)\n",
    "\n",
    "for tweet in tweets.data:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the API: users\n",
    "\n",
    "Get some info on a given user, and explore their friends/followers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Elon Musk\n",
      "------\n",
      "Following: 183\n",
      "Followers: 131391961\n",
      "Number of tweets: 23586\n",
      "------\n",
      "List of 10 followers:\n",
      "KashafR00550055\n",
      "TaifaMoni11368\n",
      "LairdLambe49446\n",
      "rexxylexi\n",
      "Josh69690799201\n",
      "plu40357\n",
      "pompompurin717\n",
      "GadgetGarry1\n",
      "thanosjuic79861\n",
      "CubixGamma\n"
     ]
    }
   ],
   "source": [
    "user = client.get_user(username=\"elonmusk\", user_fields=[\"public_metrics\"])\n",
    "\n",
    "print(\"User:\", user.data.name)\n",
    "print(\"------\")\n",
    "print(\"Following:\", user.data.public_metrics['following_count'])\n",
    "print(\"Followers:\", user.data.public_metrics['followers_count'])\n",
    "print(\"Number of tweets:\", user.data.public_metrics['tweet_count'])\n",
    "print(\"------\")\n",
    "print(\"List of 10 followers:\")\n",
    "\n",
    "users = client.get_users_followers(id=user.data.id, max_results=10)\n",
    "\n",
    "for user in users.data:\n",
    "    print(user.username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the API: tweets from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44196397\n",
      "Parents don‚Äôt realize the Soviet level of indoctrination that their children are receiving in elite high schools &amp; colleges! https://t.co/xhpmNanp6b\n",
      "@cb_doge That whole experience gave me great respect for farmers ‚Äì they work hard to put the food on everyone‚Äôs table\n",
      "@cb_doge That‚Äôs me working on my 2nd cousin‚Äôs wheat farm in Saskatchewan. I think we did a barn raising that day. \n",
      "\n",
      "If you want to see peak human labor productivity, a good old barn-raising is it!\n",
      "@cb_doge ‚ÄúTwo hats are better than one‚Äù ‚Äì wise proverb\n",
      "@wintonARK What will be left for us humans to do? We better get a move on with Neuralink!\n",
      "@alx !\n",
      "The intent is for this site to be fair &amp; impartial, favoring no party, seeking only the least wrong truth. https://t.co/DA38cflAIi\n",
      "@BillyM2k ü§î\n",
      "@kevinnbass Indeed\n",
      "@MuskUniversity I hope so\n"
     ]
    }
   ],
   "source": [
    "user = client.get_user(username=\"elonmusk\")\n",
    "print(user.data.id)\n",
    "\n",
    "elon_tweets = client.get_users_tweets(id=user.data.id)\n",
    "\n",
    "for tweet in elon_tweets.data[:10]:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those who don't have a Twitter account and app, here are some tweets on and by Boris Johnson!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mention_tweets(username, n=100):\n",
    "    \"\"\"\n",
    "    Get all tweet mentions by Twitter username.\n",
    "    \n",
    "    Args:\n",
    "        username (str): Twitter username\n",
    "        n (int, optional): number of tweets to get from timeline\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tweets (str) in which this user is mentioned\n",
    "    \"\"\"\n",
    "    \n",
    "    if username.startswith('@'):\n",
    "        user = username\n",
    "    else:\n",
    "        user = '@' + username\n",
    "    \n",
    "    mentions = []\n",
    "    tweets_on_user = tweepy.Paginator(client.search_all_tweets, query=user, max_results=n).flatten(limit=250)\n",
    "    \n",
    "    for tweet in tweets_on_user:\n",
    "        mentions.append(tweet.text)\n",
    "        \n",
    "    return mentions\n",
    "    \n",
    "    \n",
    "def get_user_tweets(username, n=100):\n",
    "    \"\"\"\n",
    "    Get a user's tweets by username.\n",
    "    \n",
    "    Args:\n",
    "        username (str): Twitter username\n",
    "        n (int, optional): number of tweets to get from timeline\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tweets (str) from this user\n",
    "    \"\"\"\n",
    "    user = client.get_user(username=username)\n",
    "    tweets_from_user = tweepy.Paginator(client.get_users_tweets, id=user.data.id, max_results=n).flatten(limit=250)\n",
    "\n",
    "    tweets = []\n",
    "    for tweet in tweets_from_user:\n",
    "        tweets.append(tweet.text)\n",
    "        \n",
    "    return tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_boris = get_mention_tweets(\"BorisJohnson\")\n",
    "from_boris = get_user_tweets(\"BorisJohnson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file (a one-column CSV)\n",
    "f_on_boris = \"stuff/tweets_on_boris.csv\"\n",
    "f_from_boris = \"stuff/tweets_from_boris.csv\"\n",
    "\n",
    "# note we are using the \"\" as text delimiter\n",
    "with open(f_on_boris, \"w\", encoding='utf-8') as f:\n",
    "    for t in on_boris:\n",
    "        f.write('\"' + t + '\"\\n')\n",
    "        \n",
    "with open(f_from_boris, \"w\", encoding='utf-8') as f:\n",
    "    for t in from_boris:\n",
    "        f.write('\"' + t + '\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.\n",
    "\n",
    "1. Download the last 100 (or another number) tweets mentioning a user you are interested into and the last 100 from the user itself. Alternatively, use the tweets in the on_boris and from_boris files.\n",
    "\n",
    "\n",
    "2. Create a minimal pipeline to normalize the tweets into lists of tokens.\n",
    "\n",
    "\n",
    "3. Count and compare from the two datasets, the most frequent (top 10):\n",
    "    - tokens\n",
    "    - hashtags\n",
    "    - other user mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
